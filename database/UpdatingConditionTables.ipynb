{
 "metadata": {
  "name": "",
  "signature": "sha256:8e3aebe8c50e0eacdbd6137b3832ae05acb96b95d866dd089d7d949934df8b27"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SQLAlchemy setup\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.sql import func, select\n",
      "from connect import mysqlusername, mysqlpassword, mysqlserver, mysqldbname\n",
      "from db_tables import metadata, ConditionDescription, ConditionSynonym, ConditionLookup, ConditionBrowse\n",
      "\n",
      "# NLP\n",
      "import nltk, codecs, string, random, math, cPickle as pickle, re, datetime, pandas as pd\n",
      "from collections import Counter, defaultdict\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# scikit-learn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "import numpy as np\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "\n",
      "from __future__ import division\n",
      "\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "stopset = set(nltk.corpus.stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load basic data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corrections = {\"Sarcoma, Ewing's\": 'Sarcoma, Ewing',\n",
      "               'Beta-Thalassemia': 'beta-Thalassemia',\n",
      "               'Von Willebrand Disease, Type 3': 'von Willebrand Disease, Type 3',\n",
      "               'Von Willebrand Disease, Type 2': 'von Willebrand Disease, Type 2',\n",
      "               'Von Willebrand Disease, Type 1': 'von Willebrand Disease, Type 1',\n",
      "               'Felty''s Syndrome': 'Felty Syndrome',\n",
      "               'Von Hippel-Lindau Disease': 'von Hippel-Lindau Disease',\n",
      "               'Retrognathism': 'Retrognathia',\n",
      "               'Regurgitation, Gastric': 'Laryngopharyngeal Reflux',\n",
      "               'Persistent Hyperinsulinemia Hypoglycemia of Infancy': 'Congenital Hyperinsulinism',\n",
      "               'Von Willebrand Diseases': 'von Willebrand Diseases',\n",
      "               'Pontine Glioma': 'Brain Stem Neoplasms',\n",
      "               'Mental Retardation': 'Intellectual Disability',\n",
      "               'Overdose': 'Drug Overdose',\n",
      "               'Beta-Mannosidosis': 'beta-Mannosidosis',\n",
      "               'Alpha 1-Antitrypsin Deficiency': 'alpha 1-Antitrypsin Deficiency',\n",
      "               'Intervertebral Disk Displacement': 'Intervertebral Disc Displacement',\n",
      "               'Alpha-Thalassemia': 'alpha-Thalassemia',\n",
      "               'Mycobacterium Infections, Atypical': 'Mycobacterium Infections, Nontuberculous',\n",
      "               'Legg-Perthes Disease': 'Legg-Calve-Perthes Disease',\n",
      "               'Intervertebral Disk Degeneration': 'Intervertebral Disc Degeneration',\n",
      "               'Alpha-Mannosidosis': 'alpha-Mannosidosis',\n",
      "               'Gestational Trophoblastic Disease': 'Gestational Trophoblastic Neoplasms'\n",
      "               }\n",
      "\n",
      "cond = defaultdict(set)\n",
      "cond_r = defaultdict(set)\n",
      "for row in codecs.open('../data/condition_browse.txt','r','utf-8').readlines():\n",
      "    row_id, trial_id, mesh_term = row.strip().split('|')\n",
      "    if mesh_term in corrections: mesh_term = corrections[mesh_term]\n",
      "    cond[mesh_term].add(trial_id)\n",
      "    cond_r[trial_id].add(mesh_term)\n",
      "\n",
      "mesh_codes = {}\n",
      "mesh_codes_r = defaultdict(set)\n",
      "for row in codecs.open('../data/mesh_thesaurus.txt','r','utf-8').readlines():\n",
      "    row_id, mesh_id, mesh_term = row.strip().split('|')\n",
      "    mesh_codes[mesh_id] = mesh_term\n",
      "    mesh_codes_r[mesh_term].add(mesh_id)\n",
      "\n",
      "# limiting to conditions that appear in ten or more trials\n",
      "top_cond = {c for c in cond if len(cond[c]) >= 10}\n",
      "trials = {t for c in top_cond for t in cond[c]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_desc = {}\n",
      "for row in codecs.open('../data/clinical_study.txt','r','utf-8').readlines():\n",
      "    data = row.split('|')\n",
      "    brief_desc, detail_desc = (data[9].replace('<br />',' '),\n",
      "                               data[10].replace('<br />',' ') if len(data[10]) > 50 else '')\n",
      "    trial_desc[data[0]] = brief_desc, detail_desc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate model guesses"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Maximum entropy model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cond_text = {c: ' '.join(' '.join(trial_desc[t]) for t in cond[c])\n",
      "             for c in top_cond}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = TfidfVectorizer(stop_words=stopset)\n",
      "train_mat = tfidf.fit_transform(cond_text.values())\n",
      "apply_mat = tfidf.transform(' '.join(trial_desc[t]) for t in trial_desc.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LogisticRegression()\n",
      "model.fit(train_mat,cond_text.keys())\n",
      "maxent_preds = dict(zip(trial_desc.keys(),model.predict(apply_mat)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(maxent_preds,open('../data/mesh_maxent.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K-Nearest Neighbors model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_text = {t: ' '.join(trial_desc[t])\n",
      "              for t in trials \n",
      "              if len(trial_desc[t][0] + trial_desc[t][1]) > 50}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = TfidfVectorizer(stop_words=stopset)\n",
      "train_mat = tfidf.fit_transform(trial_text.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(tfidf,open('../data/tfidf_model.pkl','wb'))\n",
      "pickle.dump(train_mat,open('../data/tfidf_matrix_alldesc.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neigh = NearestNeighbors(n_neighbors=10,radius=5)\n",
      "neigh.fit(train_mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# knn_preds = {}\n",
      "knn_preds = pickle.load(open('../data/mesh_knn.pkl','rb'))\n",
      "t_keys = trial_text.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(t_keys)):\n",
      "    trial_id = t_keys[i]\n",
      "    if trial_id not in knn_preds:\n",
      "        dist, idx = (arr.flatten() for arr in neigh.kneighbors(train_mat[i]))\n",
      "\n",
      "        this_guess = defaultdict(float)\n",
      "        for j in range(len(idx)):\n",
      "            k_trial_id = t_keys[idx[j]]\n",
      "            if k_trial_id != trial_id:\n",
      "                for mterm in cond_r[k_trial_id]:\n",
      "                    this_guess[mterm] += 1 / (10 ** dist[j])\n",
      "        \n",
      "        knn_preds[trial_id] = this_guess\n",
      "    \n",
      "    if i % 100 == 0: print i, datetime.datetime.now().time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(knn_preds,open('../data/mesh_knn.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "knn_accuracy = {}\n",
      "for trial_id in list(set(knn_preds.keys()) & set(cond_r.keys()) & trials):\n",
      "    # initialize variables for this prediction\n",
      "    accuracy = {'exact': 10000,\n",
      "                'hypernym': {}}\n",
      "    this_pred = knn_preds[trial_id]\n",
      "    val_order = {j: i for i, j in enumerate(sorted(list(set(this_pred.values())), reverse=True))}\n",
      "\n",
      "    hyp_pred = {m: val_order[v]\n",
      "                for p, v in this_pred.items() \n",
      "                if p in mesh_codes_r \n",
      "                for m in mesh_codes_r[p]}\n",
      "    \n",
      "    # loop through known MeSH terms to look for greatest overlap\n",
      "    for m in cond_r[trial_id]:\n",
      "        if m in this_pred:\n",
      "            this_rank = val_order[this_pred[m]]\n",
      "            if this_rank < accuracy['exact']:\n",
      "                accuracy['exact'] = this_rank\n",
      "        elif m in mesh_codes_r:\n",
      "            for a in mesh_codes_r[m]:\n",
      "                len_a = len(a)\n",
      "                for i in range(len_a,0,-4):\n",
      "                    for pa in hyp_pred:\n",
      "                        if pa[:i] == a[:i]:\n",
      "                            if i not in accuracy['hypernym'] or hyp_pred[pa] < accuracy['hypernym'][i]:\n",
      "                                accuracy['hypernym'][i] = hyp_pred[pa]\n",
      "\n",
      "    if accuracy['exact'] == 10000: accuracy['exact'] = None\n",
      "    \n",
      "    knn_accuracy[trial_id] = accuracy\n",
      "\n",
      "c = Counter([knn_accuracy[t]['exact']+1 \n",
      "             if knn_accuracy[t]['exact'] is not None\n",
      "             else 'No match'\n",
      "             for t in knn_accuracy.keys()])\n",
      "print sum(c.values()), len(knn_accuracy)\n",
      "\n",
      "ax = pd.DataFrame(c.values(), index=c.keys()).plot(kind='bar',\n",
      "                                                   figsize=(8,8),\n",
      "                                                   legend=False)\n",
      "\n",
      "ax.set_xlabel(\"Highest rank of nearest neighbor prediction exact match\")\n",
      "ax.set_ylabel(\"Number of trials\")\n",
      "ax.set_title(\"Evaluating KNN predictions using manually assigned MeSH terms:\\nHighest ranking match of a known term\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# finding good threshold for suggestions\n",
      "# based on these, going to suggest anything with a distance >= 0.4 (higher is closer)\n",
      "rank_accuracy = defaultdict(list)\n",
      "dist_accuracy = defaultdict(list)\n",
      "tr = [c for c in cond_r if len(cond_r[c]) >= 5 and c in knn_preds]\n",
      "for trial_id in tr:\n",
      "    # initialize variables for this prediction\n",
      "    accuracy = {'exact': 10000,\n",
      "                'hypernym': {}}\n",
      "    this_pred = knn_preds[trial_id]\n",
      "    this_rank = defaultdict(list)\n",
      "    this_dist = defaultdict(list)\n",
      "    val_order = {j: i for i, j in enumerate(sorted(list(set(this_pred.values())), reverse=True))}\n",
      "    \n",
      "    for p in this_pred:\n",
      "        val_round = round(this_pred[p], 2)\n",
      "        val_rank = val_order[this_pred[p]]\n",
      "        this_dist[val_round].append(1 if p in cond_r[trial_id] else 0)\n",
      "        this_rank[val_rank].append(1 if p in cond_r[trial_id] else 0)\n",
      "    \n",
      "    for v in this_dist:\n",
      "        dist_accuracy[v].append(1 if sum(this_dist[v]) > 0 else 0)\n",
      "    for v in this_rank:\n",
      "        rank_accuracy[v].append(1 if sum(this_rank[v]) > 0 else 0)\n",
      "    \n",
      "r = sorted([(r, sum(rank_accuracy[r]) / len(rank_accuracy[r])) for r in rank_accuracy], key=lambda x: x[0])\n",
      "\n",
      "ax = pd.DataFrame([t[1] for t in r], index=[t[0] for t in r]).plot(kind='bar',\n",
      "                                                   figsize=(12,8),\n",
      "                                                   legend=False)\n",
      "\n",
      "d = sorted([(r, sum(dist_accuracy[r]) / len(dist_accuracy[r])) for r in dist_accuracy], key=lambda x: x[0])\n",
      "\n",
      "ax = pd.DataFrame([t[1] for t in d], index=[t[0] for t in d]).plot(kind='bar',\n",
      "                                                   figsize=(40,8),\n",
      "                                                   legend=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Testing certain terms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_guesses(nctid):\n",
      "    these_guess = sorted(knn_preds[nctid].items(), key=lambda x: x[1], reverse=True)\n",
      "    for t, c in these_guess:\n",
      "        print '%s (%g)' % (t, c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term = 'Prostatic Neoplasms'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = [k for k in knn_preds \n",
      "     if sorted(knn_preds[k].items(), key=lambda x: x[1], reverse=True)[0][0] == term\n",
      "        and term not in cond_r[k]]\n",
      "print len(g)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#t = random.choice(g)\n",
      "t = 'NCT00487786'\n",
      "print t\n",
      "print cond_r[t]\n",
      "print\n",
      "print trial_text[t]\n",
      "print\n",
      "print_guesses(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# good one for prostate cancer: NCT00487786"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Alternate using SVD for faster neighbors lookup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.neighbors import BallTree\n",
      "tfidf = TfidfVectorizer(stop_words=stopset)\n",
      "train_mat = tfidf.fit_transform(trial_text.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svd = TruncatedSVD(n_components=1000)\n",
      "tsvd = svd.fit_transform(train_mat)\n",
      "train_mat_b = BallTree(tsvd)\n",
      "neigh = NearestNeighbors(n_neighbors=10,radius=5)\n",
      "neigh.fit(train_mat_b)\n",
      "knn_preds2 = {}\n",
      "t_keys = trial_text.keys()\n",
      "for i in range(1000):\n",
      "    trial_id = t_keys[i]\n",
      "    if trial_id not in knn_preds2:\n",
      "        dist, idx = (arr.flatten() for arr in neigh.kneighbors(tsvd[i]))\n",
      "\n",
      "        this_guess = defaultdict(float)\n",
      "        for j in range(len(idx)):\n",
      "            k_trial_id = t_keys[idx[j]]\n",
      "            if k_trial_id != trial_id:\n",
      "                for mterm in cond_r[k_trial_id]:\n",
      "                    this_guess[mterm] += 1 / (10 ** dist[j])\n",
      "        \n",
      "        knn_preds2[trial_id] = this_guess\n",
      "    \n",
      "    if i % 100 == 0: print i, datetime.datetime.now().time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Processing Medline topics and thesaurus information"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "soup = BeautifulSoup(codecs.open('../data/mplus_topics_2014-11-04.xml','r','utf-8').read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# synonyms for MeSH terms (and reverse), and topic descriptions\n",
      "mesh_syn = defaultdict(set)\n",
      "topic_desc = {}\n",
      "\n",
      "# loop through topics to pull out descriptions and synonyms\n",
      "for t in soup.find_all(\"health-topic\",language=\"English\"):\n",
      "    # topic summary\n",
      "    topic_desc[t.attrs[\"title\"]] = t.find(\"full-summary\").text.replace('\\n','').replace('\\t','')\n",
      "    \n",
      "    # MeSH synonyms\n",
      "    cur_mesh = t.find(\"mesh-heading\").descriptor.text\n",
      "    if cur_mesh in cond:\n",
      "        mesh_syn[cur_mesh] |=  set([t.attrs[\"title\"]] + [a.text for a in t.find_all(\"also-called\")])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cleanup synonyms lookup dictionary\n",
      "for m in mesh_syn.keys():\n",
      "    cur_set = mesh_syn[m].copy()\n",
      "    for s in mesh_syn[m]:\n",
      "        if m.lower() == s.lower() or len(s) == 1: \n",
      "            cur_set -= set([s])\n",
      "    if len(cur_set) == 0:\n",
      "        del(mesh_syn[m])\n",
      "    else:\n",
      "        mesh_syn[m] = cur_set\n",
      "\n",
      "for m in mesh_syn.keys():\n",
      "    for s in mesh_syn[m]:\n",
      "        if s in cond:\n",
      "            mesh_syn[s].add(m)\n",
      "            mesh_syn[s] |= mesh_syn[m]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a single MeSH term to represent the description\n",
      "king_mesh = defaultdict(set)\n",
      "not_king = set()\n",
      "for m in mesh_syn.keys():\n",
      "    if cond[m] and m not in not_king:\n",
      "        all_terms = set([m])\n",
      "        all_terms |= mesh_syn[m]\n",
      "        all_keys = [s for s in mesh_syn if m in mesh_syn[s]]\n",
      "        for k in all_keys:\n",
      "            all_terms.add(k)\n",
      "            all_terms |= mesh_syn[k]\n",
      "        top_term = sorted([(t, len(cond[t])) for t in all_terms], key=lambda x: x[1], reverse=True)[0][0]\n",
      "        king_mesh[top_term] = all_terms\n",
      "        not_king.update([t for t in all_terms if cond[t] and t != top_term])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create canonical description dictionary, linked to MeSH term\n",
      "descriptions = {}\n",
      "cond_lower = {c.lower(): c for c in king_mesh}\n",
      "for t in topic_desc:\n",
      "    \n",
      "    tlow = t.lower()\n",
      "    \n",
      "    if tlow in cond_lower:\n",
      "        descriptions[cond_lower[tlow]] = topic_desc[t]\n",
      "    \n",
      "    poss_match = [m for m in king_mesh if tlow in [n.lower() for n in king_mesh[m]]]\n",
      "    for p in poss_match:\n",
      "        descriptions[p] = topic_desc[t]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create reverse lookup to canonical MeSH term\n",
      "king_mesh_r = {s: k for k in king_mesh for s in king_mesh[k]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(king_mesh)\n",
      "print len(king_mesh_r.values())\n",
      "print len(set(king_mesh_r.values()))\n",
      "print len(descriptions.keys())\n",
      "print len(set(king_mesh.keys()) & set(descriptions.keys()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(descriptions,open('../data/mesh_descriptions.pkl','wb'))\n",
      "pickle.dump(king_mesh_r,open('../data/mesh_synonyms.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Writing data to MySQL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load guess and description dictionaries (from processing below)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxent_preds = pickle.load(open('../data/mesh_maxent.pkl','rb'))\n",
      "knn_preds = pickle.load(open('../data/mesh_knn.pkl','rb'))\n",
      "descriptions = pickle.load(open('../data/mesh_descriptions.pkl','rb'))\n",
      "synonyms = pickle.load(open('../data/mesh_synonyms.pkl','rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set up MySQL connection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mysqlserver = 'localhost'\n",
      "engine = create_engine('mysql://%s:%s@%s/%s' % (mysqlusername, mysqlpassword, mysqlserver, mysqldbname), pool_recycle=3600)\n",
      "conn = engine.connect()\n",
      "metadata.create_all(engine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Upload canonical condition description and synonym data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add conditions that aren't already in descriptions dictionary\n",
      "for c in cond:\n",
      "    if c not in descriptions:\n",
      "        if c in synonyms:\n",
      "            descriptions[c] = descriptions[synonyms[c]]\n",
      "        else:\n",
      "            descriptions[c] = ''\n",
      "\n",
      "for s in synonyms:\n",
      "    if s not in descriptions:\n",
      "        descriptions[s] = descriptions[synonyms[s]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "desc_id = {k: i for i, k in enumerate(descriptions.keys())}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn.execute(ConditionDescription.insert(), [{'condition_id': desc_id[d],\n",
      "                                              'mesh_term': d,\n",
      "                                              'description': descriptions[d]}\n",
      "                                             for d in descriptions.keys()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create lookup for every synonym\n",
      "syn_rev = {k: set(s for s, j in synonyms.items() if j == k) for k in set(synonyms.values())}\n",
      "all_syn = [{'condition_id': desc_id[s],\n",
      "            'synonym_id': desc_id[k]}\n",
      "           for s in synonyms\n",
      "           for k in syn_rev[synonyms[s]]\n",
      "           if s != k]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn.execute(ConditionSynonym.insert(), all_syn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Insert conditions already labeled in the database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(0,len(cond),100):\n",
      "    print k\n",
      "    conn.execute(ConditionLookup.insert(), [{'condition_id': desc_id[c],\n",
      "                                             'nct_id': n,\n",
      "                                             'source': 'CTGOV',\n",
      "                                             'syn_flag': 0}\n",
      "                                            for c in cond.keys()[k:k+100]\n",
      "                                            for n in cond[c]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Insert maximum entropy predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(0,len(maxent_preds),5000):\n",
      "    print k\n",
      "    conn.execute(ConditionLookup.insert(), [{'condition_id': desc_id[t],\n",
      "                                             'nct_id': n,\n",
      "                                             'source': 'MAXENT',\n",
      "                                             'syn_flag': 0}\n",
      "                                            for n, t in maxent_preds.items()[k:k+5000]\n",
      "                                            if t not in cond_r[n]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Insert KNN predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "knn_insert = []\n",
      "for k in knn_preds:\n",
      "    this_pred = knn_preds[k]\n",
      "    val_order = {j: i for i, j in enumerate(sorted(list(set(this_pred.values())), reverse=True))}\n",
      "    for m in this_pred:\n",
      "        if m not in cond_r[k] and (this_pred[m] >= 0.4 or val_order[this_pred[m]] == 0):\n",
      "            knn_insert.append({'condition_id': desc_id[m],\n",
      "                               'nct_id': k,\n",
      "                               'source': 'KNN',\n",
      "                               'disp_order': val_order[this_pred[m]],\n",
      "                               'syn_flag': 0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(0,len(knn_insert),5000):\n",
      "    print k\n",
      "    conn.execute(ConditionLookup.insert(), knn_insert[k:k+5000])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Insert synonyms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for s, k in synonyms.items():\n",
      "    \n",
      "    if s != k:\n",
      "        \n",
      "        k_trials = {t[0] for t in conn.execute('''select nct_id \n",
      "                                                  from condition_lookup \n",
      "                                                  where condition_id = %d\n",
      "                                                    and source = \"CTGOV\"''' % desc_id[k]).fetchall()}\n",
      "        s_trials = {t[0] for t in conn.execute('''select nct_id \n",
      "                                                  from condition_lookup\n",
      "                                                  where condition_id = %d\n",
      "                                                    and source = \"CTGOV\"''' % desc_id[s]).fetchall()}\n",
      "        \n",
      "        s_insert = [{'condition_id': desc_id[s],\n",
      "                     'nct_id': t,\n",
      "                     'source': 'CTGOV',\n",
      "                     'syn_flag': 1}\n",
      "                    for t in k_trials - s_trials]\n",
      "        \n",
      "        if cond[s]:\n",
      "            k_insert = [{'condition_id': desc_id[k],\n",
      "                         'nct_id': t,\n",
      "                         'source': 'CTGOV',\n",
      "                         'syn_flag': 1}\n",
      "                        for t in s_trials - k_trials]\n",
      "        else:\n",
      "            k_insert = []\n",
      "        \n",
      "        conn.execute(ConditionLookup.insert(), s_insert + k_insert)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}