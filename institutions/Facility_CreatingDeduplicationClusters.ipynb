{
 "metadata": {
  "gist_id": "eee672d26aa9ef6ccd01",
  "name": "",
  "signature": "sha256:7ead0ac23613d5cca6da3e7db9e373799d46121e72ca3bc7894e94175710a014"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs, nltk, json, string, cPickle as pickle, random, collections, dedupe, numpy as np, itertools, time, re\n",
      "import pandas as pd\n",
      "from string import punctuation\n",
      "pd.set_option('display.max_rows', 500)\n",
      "\n",
      "from sqlalchemy import create_engine\n",
      "from connect import mysqlusername, mysqlpassword, mysqlserver, mysqldbname"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jost/courses/clinicaltrials/env/lib/python2.7/site-packages/dedupe/backport.py:12: UserWarning: NumPy linked against 'Accelerate.framework'. Multiprocessing will be disabled. http://mail.scipy.org/pipermail/numpy-discussion/2012-August/063589.html\n",
        "  warnings.warn(\"NumPy linked against 'Accelerate.framework'. \"\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Get facility data and prepare list to dedupe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mysqlserver = 'localhost'\n",
      "mysqldbname = 'clinicaltrials2'\n",
      "conn = create_engine('mysql://%s:%s@%s/%s' % (mysqlusername, mysqlpassword, mysqlserver, mysqldbname))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "column_names = [\"facility_id\",\n",
      "                \"nct_id\",\n",
      "                \"status\",\n",
      "                \"facility_name\",\n",
      "                \"city\",\n",
      "                \"state\",\n",
      "                \"zipcode\",\n",
      "                \"country\"]\n",
      "\n",
      "facilities = pd.read_csv('../data/facilities.txt', names=column_names, sep=\"|\", encoding='utf-8', quoting=3)\n",
      "\n",
      "for c in column_names[2:]:\n",
      "    facilities[c] = facilities[c].apply(lambda x: x if pd.notnull(x) else u'')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states = facilities[facilities.country == 'United States'].groupby(facilities.state).count().to_dict()['state'].keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "bad_names = [r'investigat[a-z]+ site',\n",
      "             r'research site',\n",
      "             r'research facility',\n",
      "             r'local institution',\n",
      "             r'study site',\n",
      "             r'clinical site',\n",
      "             r'call center',\n",
      "             r'site ref',\n",
      "             r'site[ :]+#?[0-9]+',\n",
      "             r'^#?[0-9\\.]+$',\n",
      "             r'for additional information',\n",
      "             r'call for information',\n",
      "             r'the study is '\n",
      "             ]\n",
      "\n",
      "potential = facilities[(facilities.country == 'United States') & \n",
      "                       (facilities.facility_name.apply(lambda x: x != u'' and min([not re.search(b,x.lower()) for b in bad_names])))][['facility_id','facility_name','city','state','zipcode','country']].drop_duplicates(['facility_name','city','state','zipcode','country'])\n",
      "\n",
      "potential.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "(101559, 6)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load model information and get blocking data ready"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "settings_file = '../data/dedupe_settings2'\n",
      "deduper = dedupe.StaticDedupe(open(settings_file,'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:Learned Weights\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:('(city: ShortString)', -0.4464088976383209)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:('(facility_name: String)', -1.2190216779708862)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:('(zipcode: ShortString)', -0.49488386511802673)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:('(state: ShortString)', -0.48534145951271057)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:('((zipcode: ShortString): Not Missing)', 0.14800745248794556)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:('((city: ShortString): Not Missing)', 1.1579594612121582)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:('bias', 2.345712661743164)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:[CompoundPredicate: (SimplePredicate: (nearIntegersPredicate, zipcode), SimplePredicate: (sameSevenCharStartPredicate, facility_name)), CompoundPredicate: (SimplePredicate: (wholeFieldPredicate, zipcode), TfidfPredicate: (0.8, facility_name)), CompoundPredicate: (SimplePredicate: (sameThreeCharStartPredicate, zipcode), SimplePredicate: (sameSevenCharStartPredicate, facility_name)), CompoundPredicate: (SimplePredicate: (wholeFieldPredicate, facility_name), SimplePredicate: (tokenFieldPredicate, city)), CompoundPredicate: (SimplePredicate: (tokenFieldPredicate, zipcode), TfidfPredicate: (0.2, facility_name)), CompoundPredicate: (SimplePredicate: (wholeFieldPredicate, zipcode), TfidfPredicate: (0.6, facility_name)), CompoundPredicate: (SimplePredicate: (sameSevenCharStartPredicate, city), SimplePredicate: (firstTokenPredicate, facility_name)), CompoundPredicate: (SimplePredicate: (sameSevenCharStartPredicate, city), TfidfPredicate: (0.2, facility_name)), CompoundPredicate: (SimplePredicate: (firstIntegerPredicate, zipcode), SimplePredicate: (sameThreeCharStartPredicate, facility_name)), CompoundPredicate: (TfidfPredicate: (0.8, facility_name), SimplePredicate: (sameSevenCharStartPredicate, facility_name)), CompoundPredicate: (TfidfPredicate: (0.4, facility_name), SimplePredicate: (wholeFieldPredicate, city)), CompoundPredicate: (TfidfPredicate: (0.6, facility_name), SimplePredicate: (sameSevenCharStartPredicate, facility_name)), CompoundPredicate: (SimplePredicate: (firstIntegerPredicate, zipcode), SimplePredicate: (commonSixGram, facility_name)), CompoundPredicate: (TfidfPredicate: (0.2, facility_name), SimplePredicate: (commonFourGram, zipcode)), CompoundPredicate: (SimplePredicate: (sameSevenCharStartPredicate, city), SimplePredicate: (sameThreeCharStartPredicate, facility_name)), CompoundPredicate: (TfidfPredicate: (0.4, facility_name), SimplePredicate: (sameSevenCharStartPredicate, facility_name)), CompoundPredicate: (SimplePredicate: (tokenFieldPredicate, zipcode), SimplePredicate: (sameSevenCharStartPredicate, state)), CompoundPredicate: (SimplePredicate: (tokenFieldPredicate, facility_name), SimplePredicate: (sameSevenCharStartPredicate, city)), CompoundPredicate: (SimplePredicate: (tokenFieldPredicate, facility_name), SimplePredicate: (nearIntegersPredicate, zipcode))]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.api:{'facility_name': set([u'and', u'Oncology', u'Center', u'Cancer', u'Institute', u'of', u'University', u'Research', u's', u'Associates', u'Health', u'Hospital', u'Clinical', u'Medical'])}\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "potential_indexed = potential.set_index('facility_id')\n",
      "\n",
      "def candidates_gen(result_set) :\n",
      "    \n",
      "    block_key = None\n",
      "    records = []\n",
      "    for row in result_set :\n",
      "        if row['block_key'] != block_key :\n",
      "            if records:\n",
      "                yield tuple(records)\n",
      "            \n",
      "            block_key = row['block_key']\n",
      "            records = []\n",
      "            \n",
      "        records.append((row['facility_id'], row, set([])))\n",
      "    \n",
      "    if records :\n",
      "        yield records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clustering by state"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dupes = {}\n",
      "for field in deduper.blocker.tfidf_fields :\n",
      "    field_data = potential[['facility_id',field]].T.to_dict().values()\n",
      "    deduper.blocker.tfIdfBlock(field_data, field)\n",
      "    \n",
      "for s in states:\n",
      "    print s\n",
      "    \n",
      "    full_data = [(f, potential_indexed.loc[f].to_dict()) for f in potential_indexed[potential_indexed.state == s].index]\n",
      "    blocks = [(i, j[0], j[1]) for i, j in enumerate(sorted(list(deduper.blocker(full_data))))]\n",
      "    \n",
      "    bkfreq = nltk.FreqDist([bk for i, bk, f in blocks])\n",
      "    \n",
      "    c = [dict([(k, v if pd.notnull(v) else u'') \n",
      "               for k, v in potential_indexed.loc[f].to_dict().items()] + [('block_key', bk), ('facility_id', f)]) \n",
      "         for i, bk, f in blocks if bkfreq[bk] > 1]\n",
      "    \n",
      "    clustered_dupes = deduper.matchBlocks(candidates_gen(c), .5)\n",
      "    print '%d clusters' % len(clustered_dupes)\n",
      "    print \n",
      "    \n",
      "    dupes[s] = clustered_dupes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.blocking:Tue Nov 25 21:38:42 2014\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.blocking:Canopy: TfidfPredicate: (0.4, facility_name)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.blocking:Canopy: TfidfPredicate: (0.6, facility_name)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.blocking:Canopy: TfidfPredicate: (0.8, facility_name)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.blocking:Canopy: TfidfPredicate: (0.2, facility_name)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.blocking:Tue Nov 25 21:38:42 2014\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mississippi\n",
        "61 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Oklahoma\n",
        "161 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Delaware\n",
        "32 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Minnesota\n",
        "223 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Illinois\n",
        "520 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Arkansas\n",
        "116 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "New Mexico\n",
        "64 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Indiana\n",
        "305 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Maryland\n",
        "362 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Louisiana\n",
        "235 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Idaho\n",
        "62 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Wyoming\n",
        "3 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Tennessee\n",
        "369 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Arizona\n",
        "324 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Iowa\n",
        "104 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Michigan\n",
        "403 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Kansas\n",
        "140 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Utah\n",
        "136 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Virginia\n",
        "359 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Oregon\n",
        "196 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Connecticut\n",
        "232 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Montana\n",
        "44 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:dedupe.blocking:10000, 1.2280222 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "California\n",
        "1834 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Massachusetts\n",
        "399 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "West Virginia\n",
        "45 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "South Carolina\n",
        "234 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "New Hampshire\n",
        "49 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Wisconsin\n",
        "232 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Vermont\n",
        "40 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Georgia\n",
        "446 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "North Dakota\n",
        "51 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Pennsylvania\n",
        "665 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Florida\n",
        "1243 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Alaska\n",
        "15 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Kentucky\n",
        "164 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Hawaii\n",
        "50 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Nebraska\n",
        "114 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Missouri\n",
        "329 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Ohio\n",
        "561 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Alabama\n",
        "209 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "New York\n",
        "977 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "South Dakota\n",
        "41 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Colorado\n",
        "335 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "New Jersey\n",
        "386 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Washington\n",
        "323 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "North Carolina\n",
        "530 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "District of Columbia\n",
        "85 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Texas\n",
        "1078 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Nevada\n",
        "104 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Maine\n",
        "42 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Rhode Island\n",
        "78 clusters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(dupes, open('../data/clustered_dupes_state.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}