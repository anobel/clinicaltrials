{
 "metadata": {
  "name": "",
  "signature": "sha256:89d4fdd4b4829ebb1b5b828a029958bd1ffb8a502013c559bbfa31856a29b3ae"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# basic NLP\n",
      "import nltk, codecs, string, random, math, cPickle as pickle, re, datetime\n",
      "from collections import Counter\n",
      "\n",
      "# scikit-learn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import numpy as np\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "\n",
      "from __future__ import division\n",
      "\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "stopset = set(nltk.corpus.stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corrections = {\"Sarcoma, Ewing's\": 'Sarcoma, Ewing',\n",
      "               'Beta-Thalassemia': 'beta-Thalassemia',\n",
      "               'Von Willebrand Disease, Type 3': 'von Willebrand Disease, Type 3',\n",
      "               'Von Willebrand Disease, Type 2': 'von Willebrand Disease, Type 2',\n",
      "               'Von Willebrand Disease, Type 1': 'von Willebrand Disease, Type 1',\n",
      "               'Felty''s Syndrome': 'Felty Syndrome',\n",
      "               'Von Hippel-Lindau Disease': 'von Hippel-Lindau Disease',\n",
      "               'Retrognathism': 'Retrognathia',\n",
      "               'Regurgitation, Gastric': 'Laryngopharyngeal Reflux',\n",
      "               'Persistent Hyperinsulinemia Hypoglycemia of Infancy': 'Congenital Hyperinsulinism',\n",
      "               'Von Willebrand Diseases': 'von Willebrand Diseases',\n",
      "               'Pontine Glioma': 'Brain Stem Neoplasms',\n",
      "               'Mental Retardation': 'Intellectual Disability',\n",
      "               'Overdose': 'Drug Overdose',\n",
      "               'Beta-Mannosidosis': 'beta-Mannosidosis',\n",
      "               'Alpha 1-Antitrypsin Deficiency': 'alpha 1-Antitrypsin Deficiency',\n",
      "               'Intervertebral Disk Displacement': 'Intervertebral Disc Displacement',\n",
      "               'Alpha-Thalassemia': 'alpha-Thalassemia',\n",
      "               'Mycobacterium Infections, Atypical': 'Mycobacterium Infections, Nontuberculous',\n",
      "               'Legg-Perthes Disease': 'Legg-Calve-Perthes Disease',\n",
      "               'Intervertebral Disk Degeneration': 'Intervertebral Disc Degeneration',\n",
      "               'Alpha-Mannosidosis': 'alpha-Mannosidosis',\n",
      "               'Gestational Trophoblastic Disease': 'Gestational Trophoblastic Neoplasms'\n",
      "               }\n",
      "cond = {}\n",
      "cond_r = {}\n",
      "for row in codecs.open('../data/condition_browse.txt','r','utf-8').readlines():\n",
      "    row_id, trial_id, mesh_term = row.strip().split('|')\n",
      "    if mesh_term in corrections: mesh_term = corrections[mesh_term]\n",
      "    if mesh_term not in cond: cond[mesh_term] = []\n",
      "    cond[mesh_term].append(trial_id)\n",
      "    if trial_id not in cond_r: cond_r[trial_id] = []\n",
      "    cond_r[trial_id].append(mesh_term)\n",
      "\n",
      "mesh_codes = {}\n",
      "mesh_codes_r = {}\n",
      "for row in codecs.open('../data/mesh_thesaurus.txt','r','utf-8').readlines():\n",
      "    row_id, mesh_id, mesh_term = row.strip().split('|')\n",
      "    mesh_codes[mesh_id] = mesh_term\n",
      "    if mesh_term not in mesh_codes_r: mesh_codes_r[mesh_term] = []\n",
      "    mesh_codes_r[mesh_term].append(mesh_id)\n",
      "\n",
      "# limiting to conditions that appear in ten or more trials\n",
      "top_cond = {c for c in cond if len(cond[c]) >= 10}\n",
      "trials = {t for c in top_cond for t in cond[c]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_desc = {}\n",
      "for row in codecs.open('../data/clinical_study.txt','r','utf-8').readlines():\n",
      "    data = row.split('|')\n",
      "    brief_desc, detail_desc = (data[9],\n",
      "                               data[10] if len(data[10]) > 50 else '')\n",
      "    trial_desc[data[0]] = brief_desc, detail_desc\n",
      "\n",
      "to_classify = [t for t in trial_desc if t not in trials]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(trial_desc,open('../data/trial_desc.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_desc = pickle.load(open('../data/trial_desc.pkl','rb'))\n",
      "to_classify = [t for t in trial_desc if t not in trials]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Analyze data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Total MeSH terms: %d' % len(cond)\n",
      "print 'Total MeSH terms (level 1): %d' % len([mesh_codes[m] for m in set([mr[:3] for c in cond  if c in mesh_codes_r for mr in mesh_codes_r[c]])])\n",
      "print 'Total MeSH terms (level 2): %d' % len([mesh_codes[m] for m in set([mr[:7] for c in cond  if c in mesh_codes_r for mr in mesh_codes_r[c]])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create trial lookup for MeSH term hypernyms in the second level of the hierarchy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cond_l2 = {}\n",
      "for m in cond.keys():\n",
      "    if m in mesh_codes_r:\n",
      "        m_l2 = set([mr[:7] for mr in mesh_codes_r[m]])\n",
      "        for l2 in m_l2:\n",
      "            if l2 not in cond_l2: cond_l2[l2] = set()\n",
      "            cond_l2[l2] |= set(cond[m])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Process text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_text(text):\n",
      "    return [word.lower() \n",
      "            for sent in sent_tokenizer.tokenize(text) \n",
      "            for word in nltk.word_tokenize(sent)\n",
      "            if word.lower() not in stopset and\n",
      "            sum(1 for char in word if char not in string.punctuation) > 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cond_text = {cond: Counter([word\n",
      "                            for trial_id in cond_l2[cond] \n",
      "                            for desc in trial_desc[trial_id]\n",
      "                            if len(desc) > 0\n",
      "                            for word in process_text(desc)])\n",
      "             for cond in cond_l2.keys()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total_text = sum(cond_text.values(),Counter())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(cond_text,open('../data/mesh_level2_textcount.pkl','wb'))\n",
      "pickle.dump(total_text,open('../data/mesh_level2_alltextcount.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cond_text = pickle.load(open('../data/mesh_level2_textcount.pkl','rb'))\n",
      "total_text = pickle.load(open('../data/mesh_level2_alltextcount.pkl','rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Building series of individual level-2 MeSH classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initializing values\n",
      "mesh_models = {}\n",
      "\n",
      "total_text_keys, total_text_values = zip(*[(k, v)\n",
      "                                           for k, v in total_text.items() \n",
      "                                           if len(k) > 2 and sum([1 \n",
      "                                                                  for char in k \n",
      "                                                                  if char not in '1234567890']) > 0])\n",
      "\n",
      "other_text_len = sum(total_text_values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = len(mesh_models) + 1\n",
      "\n",
      "for c in cond_text.keys():\n",
      "    if c not in mesh_models and len(c) > 3:\n",
      "        # get total number of words for that term and for everything else that isn't that term\n",
      "        cond_text_len = sum([v \n",
      "                             for k, v in cond_text[c].items() \n",
      "                             if len(k) > 2 and sum([1 \n",
      "                                                    for char in k \n",
      "                                                    if char not in '1234567890']) > 0])\n",
      "        cur_other_text_len = other_text_len - cond_text_len\n",
      "        \n",
      "        # create set of tuples (term % of target MeSH descriptor text, term % of other MeSH descriptor text)\n",
      "        vecs = [(cond_text[c][t] / cond_text_len, (total_text[t] - cond_text[c][t]) / cur_other_text_len)\n",
      "                for t in total_text.keys()\n",
      "                if len(t) > 2 and sum([1\n",
      "                                       for char in t\n",
      "                                       if char not in '1234567890']) > 0]\n",
      "\n",
      "        # fit logistic model\n",
      "        model = LogisticRegression()\n",
      "        mesh_models[c] = model.fit(zip(*vecs),[1,0])\n",
      "\n",
      "        print '%-3d %s (%s)' % (i, c, mesh_codes[c])\n",
      "        i += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(mesh_models,open('../data/mesh_models_series.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mesh_models = pickle.load(open('../data/mesh_models_series.pkl','rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Applying models to each unclassified trial"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classify_text = {trial_id: Counter([word\n",
      "                                    for desc in trial_desc[trial_id]\n",
      "                                    if len(desc) > 0\n",
      "                                    for word in process_text(desc)])\n",
      "                 for trial_id in to_classify}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "guesses = {}\n",
      "total_text_keys, total_text_values = zip(*[(k, v)\n",
      "                                           for k, v in total_text.items() \n",
      "                                           if len(k) > 2 and sum([1 \n",
      "                                                                  for char in k \n",
      "                                                                  if char not in '1234567890']) > 0])\n",
      "\n",
      "other_text_len = sum(total_text_values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = len(guesses) + 1\n",
      "\n",
      "for c in classify_text.keys():\n",
      "    if c not in guesses:\n",
      "        text_len = sum([v\n",
      "                        for k, v in classify_text[c].items()\n",
      "                        if len(k) > 2 and sum([1\n",
      "                                               for char in k\n",
      "                                               if char not in '1234567890']) > 0])\n",
      "        \n",
      "        if text_len > 0:\n",
      "            # create set of tuples (term % of target descriptor text, term % of other MeSH descriptor text)\n",
      "            vecs = [classify_text[c][t] / text_len\n",
      "                    for t in total_text.keys()\n",
      "                    if len(t) > 2 and sum([1\n",
      "                                           for char in t\n",
      "                                           if char not in '1234567890']) > 0]\n",
      "\n",
      "            # predict logistic models\n",
      "            predictions = {}\n",
      "            for term, model in mesh_models.items():\n",
      "                predictions[term] = model.predict_proba(vecs)[0][1]\n",
      "\n",
      "            guesses[c] = predictions\n",
      "\n",
      "        i += 1\n",
      "        if i % 10 == 0: print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(guesses,open('../data/mesh_guesses.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Single-prediction maxent classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cond_text = {c: ' '.join(' '.join(trial_desc[t]) for t in cond[c])\n",
      "             for c in top_cond}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = TfidfVectorizer(stop_words=stopset)\n",
      "train_mat = tfidf.fit_transform(cond_text.values())\n",
      "apply_mat = tfidf.transform(' '.join(trial_desc[t]) for t in to_classify)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LogisticRegression()\n",
      "model.fit(train_mat,cond_text.keys())\n",
      "single_preds = dict(zip(to_classify,model.predict(apply_mat)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(single_preds,open('../data/mesh_guesses_maxent.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## K Nearest Neighbors suggestions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_text = {t: ' '.join(trial_desc[t])\n",
      "              for t in trials \n",
      "              if len(trial_desc[t][0] + trial_desc[t][1]) > 50}\n",
      "trial_text_other = {t: ' '.join(trial_desc[t]) \n",
      "                    for t in to_classify\n",
      "                    if len(trial_desc[t][0] + trial_desc[t][1]) > 50}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = TfidfVectorizer(stop_words=stopset)\n",
      "train_mat = tfidf.fit_transform(trial_text.values())\n",
      "apply_mat = tfidf.transform(trial_text_other.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import NearestNeighbors\n",
      "neigh = NearestNeighbors(n_neighbors=10,radius=5)\n",
      "neigh.fit(train_mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "         n_neighbors=10, radius=5)"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "knn_guesses = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(trial_text_other.keys())):\n",
      "    trial_id = trial_text_other.keys()[i]\n",
      "    if trial_id not in knn_guesses:\n",
      "        dist, idx = (arr.flatten() for arr in neigh.kneighbors(apply_mat[i]))\n",
      "\n",
      "        this_guess = {}\n",
      "        for j in range(len(idx)):\n",
      "            k_trial_id = trial_text.keys()[idx[j]]\n",
      "            for mterm in cond_r[k_trial_id]:\n",
      "                if mterm not in this_guess: this_guess[mterm] = []\n",
      "                this_guess[mterm].append(dist[j])\n",
      "\n",
      "        knn_guesses[trial_id] = this_guess\n",
      "    if i % 100 == 0: print i, datetime.datetime.now().time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 15:10:31.421026\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:31.582154\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:31.735934\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:31.890507\n",
        "400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:32.062824\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:32.214026\n",
        "600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:32.384057\n",
        "700"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:32.570245\n",
        "800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:32.742485\n",
        "900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:32.906512\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:33.067684\n",
        "1100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:33.229257\n",
        "1200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15:10:33.386769\n",
        "1300"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(knn_guesses,open('../data/mesh_guesses_knn.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}