{
 "metadata": {
  "name": "",
  "signature": "sha256:023519a87f2b19a16202522c21672534ec9ec71c78f3fee9eaa0b0d229002904"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import codecs\n",
      "import json\n",
      "import string\n",
      "import pandas as pd\n",
      "from string import punctuation\n",
      "import cPickle as pickle\n",
      "import os\n",
      "import re\n",
      "import unicodedata\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Investigator Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "column_names = ['Investigator_id', \"facility_id\", \"nct_id\", 'Name', 'Investigator_Type', 'Null']\n",
      "\n",
      "investigators_df = pd.read_csv('../data/investigators.txt', names=column_names, sep=\"|\", encoding='utf-8', quoting=3)\n",
      "investigators_df = investigators_df[investigators_df.columns[:-1]]\n",
      "\n",
      "# remove everything after the comma in names to get rid of Dr. ect\n",
      "investigators_df.Name = investigators_df.Name.apply(lambda x: x.split(',')[0])\n",
      "\n",
      "#different number of unique ids to unique names\n",
      "print '# of unique facility ids: ', len(set(investigators_df.Investigator_id))\n",
      "\n",
      "print '# of unique investigator names: ', len(set(investigators_df.Name))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# of unique facility ids:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "147021\n",
        "# of unique investigator names:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "98129\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(investigators_df.Investigator_id))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "147021"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Invesigator Id actually = facility id"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get a dict of all unique investigator names associated with their ids and studies\n",
      "investigator_dict = {}\n",
      "duplicate = []\n",
      "for invest, data in investigators_df.groupby('Name'):\n",
      "#     if len(set(data.Investigator_id)) > 1:\n",
      "#         duplicate.append(data)\n",
      "    #remove Site Reference ID's\n",
      "    if 'Site Reference ID' in list(data.Name)[0]:\n",
      "        continue\n",
      "    investigator_dict[list(data.Name)[0]] = {'id': list(set(data.Investigator_id)), 'Trials':list(set(data.nct_id))}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(investigator_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "91029"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "investigator_dict['Thasarat Vajaranant']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "{'Trials': [u'NCT01630551'], 'id': [80886]}"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save investigator dict\n",
      "pickle.dump(investigator_dict, open('investigator_dict.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(map(lambda x: x.lower(), investigator_dict.keys())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Pull publications"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load in already run investigators\n",
      "done = set()\n",
      "files = os.listdir('../data')\n",
      "for f in files:\n",
      "    if 'investigator_dict_' in f:\n",
      "        done.update(pickle.load(open('../data/' + f,'rb')).keys())\n",
      "\n",
      "listnum = len([f for f in files if 'investigator_dict_' in f]) + 1\n",
      "\n",
      "#break up the list of names into 5 chuncks\n",
      "todo = [i for i in investigator_dict.keys() if i not in done][:20000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_publications(name_list, list_num):\n",
      "    count=0\n",
      "    result_dict = {}\n",
      "    for name in name_list:\n",
      "        start_time = time.time()\n",
      "        #get publication ids for investigator\n",
      "        search_start = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed'\n",
      "        investigator = name\n",
      "\n",
      "        r = requests.get(search_start + '&term=%s[Author]&retmax=100000' % investigator)\n",
      "        soup = BeautifulSoup(r.text)\n",
      "        ids = [s.text for s in soup.find_all('id')]\n",
      "        \n",
      "        #don't do second call if there are no ids\n",
      "        if len(ids) == 0:\n",
      "            continue\n",
      "        \n",
      "        #get publications from ids\n",
      "        summary_start = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed'\n",
      "        id_list = ','.join(ids)\n",
      "\n",
      "        r = requests.get(summary_start + '&id=%s&retmode=xml&retmax=10000' % id_list)\n",
      "\n",
      "        #pickle does not work with soup objects\n",
      "        #soup = BeautifulSoup(r.text)\n",
      "        \n",
      "        #add publications to the dict\n",
      "        result_dict[name] = ' '.join(r.text.split())\n",
      "        \n",
      "        #time taken\n",
      "        if (time.time() - start_time) < 1:\n",
      "            time.sleep(1-(time.time() - start_time))\n",
      "\n",
      "        count += 1\n",
      "\n",
      "        if (count % 500) == 0 or count == len(name_list):\n",
      "            pickle.dump(result_dict, open('investigator_dict_%d.pkl' % (list_num),'wb'))\n",
      "            print count\n",
      "            list_num += 1\n",
      "            result_dict = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pub_dict = get_publications(todo, listnum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}