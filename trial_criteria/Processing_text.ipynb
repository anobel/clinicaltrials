{
 "metadata": {
  "name": "",
  "signature": "sha256:757134a8ed0e76a022f8333f6a864bd67b03c012efa1e12d364d08f883594658"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import codecs\n",
      "import string\n",
      "import random\n",
      "from collections import Counter\n",
      "import cPickle as pickle\n",
      "import re\n",
      "from copy import deepcopy\n",
      "import csv\n",
      "import pandas as pd\n",
      "\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook was for developing the programs that had to be run on harbinger to process the full corpus of text. Because of the size it was too computationally intensive to run on my computer (the pos tagging took around 11 hours on harbinger to complete)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria = {}\n",
      "predefined_criteria = {}\n",
      "for row in codecs.open('data/clinical_study.txt','r','utf-8').readlines():\n",
      "    data = row.split('|')\n",
      "    trial_criteria[data[0]] = data[26]\n",
      "    # 27 = gender, 28 = min age, 29 = max age, 30 = Health volunteers\n",
      "    predefined_criteria[data[0]] = (data[27], data[28], data[29], data[30])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prepare text for Stanford Tagger"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Create list and lookup dict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stanford_list(trial_criteria):\n",
      "    #create a flat list of all sentences for the stanford tagger to run on\n",
      "    #also creating a look up dict where the index of the sentence in the \n",
      "    #stanford tagger list is associated with the trial it came from\n",
      "    stanford_sentence_list = []\n",
      "    sentence_lookup_dict = {}\n",
      "    for trial, criteria in trial_criteria.items():\n",
      "        stan_len = len(stanford_sentence_list)\n",
      "        new_list_len = len(criteria.split('<br />'))\n",
      "        stanford_sentence_list += criteria.split('<br />')\n",
      "        for ix in xrange(stan_len, stan_len + new_list_len, 1):\n",
      "            sentence_lookup_dict[ix] = trial\n",
      "    return stanford_sentence_list, sentence_lookup_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stanford_sentence_list, sentence_lookup_dict = stanford_list(trial_criteria)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Remove bullet point symobols"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_bullet_points(stanford_sentence_list):\n",
      "    '''Remove bullet point symbols from text'''\n",
      "    for ix, sent in enumerate(stanford_sentence_list):\n",
      "        m = re.search('^-?[0-9]?.?\\)?\\s', sent)\n",
      "        if m:\n",
      "            stanford_sentence_list[ix] = sent[len(m.group(0)):]\n",
      "    return stanford_sentence_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stanford_sentence_list = clean_bullet_points(stanford_sentence_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save Stanford preped text and lookup dict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.DataFrame(stanford_sentence_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_csv('data/stanford_sentence_list.csv', header=False, index=False, encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(sentence_lookup_dict, open('data/sentence_lookup_dict.pkl', 'wb'))\n",
      "\n",
      "\n",
      "\n",
      "# myfile = open('data/stanford_sentence_list.csv', 'wb')\n",
      "# wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
      "# wr.writerow(stanford_sentence_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeEncodeError",
       "evalue": "'ascii' codec can't encode character u'\\u2264' in position 41: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-90-8f6904720170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/stanford_sentence_list.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_ALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstanford_sentence_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u2264' in position 41: ordinal not in range(128)"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Split Inclusive and Exclusive"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_doc_on_exclusion(doc):\n",
      "    '''This function splits the criteria documents into the Inclusion \n",
      "    and Exclusion sections'''\n",
      "    split_doc = doc.split()\n",
      "    for idx, word in enumerate(split_doc):\n",
      "        if word.lower() == u'exclusion' and u'criteria' in split_doc[idx + 1].lower():\n",
      "            inclusive = ' '.join(split_doc[:idx])\n",
      "            exclusive =  ' '.join(split_doc[idx:])\n",
      "            return inclusive, exclusive\n",
      "    if split_doc[0].lower() == u'inclusion' and u'criteria' in split_doc[1].lower():\n",
      "        return doc, None\n",
      "\n",
      "no_inc_exc_dict = {}\n",
      "trial_criteria_split = {}\n",
      "\n",
      "for key, doc in trial_criteria.items():\n",
      "    try:\n",
      "        inclusive, exclusive = split_doc_on_exclusion(doc)\n",
      "        if exclusive is None:\n",
      "            trial_criteria_split[key] = [inclusive]\n",
      "        else:\n",
      "            trial_criteria_split[key] = [inclusive, exclusive]\n",
      "            \n",
      "    #if there are not inclusion/exclusion\n",
      "    except:\n",
      "         no_inc_exc_dict[key] = doc\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(trial_criteria_split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "152877"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save ram\n",
      "del trial_criteria"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "~10,000 trials did not have an inclusive exclusive split"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(no_inc_exc_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "10887"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(trial_criteria)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "163764"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save to pickle\n",
      "pickle.dump(predefined_criteria, open('data/predefined_criteria.pkl', 'wb'))\n",
      "pickle.dump(trial_criteria, open('data/trial_criteria.pkl', 'wb'))\n",
      "pickle.dump(trial_criteria_split, open('data/trial_criteria_split.pkl', 'wb'))\n",
      "pickle.dump(no_inc_exc_dict, open('data/no_inc_exc_dict.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data From Pickle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria = pickle.load(open('data/trial_criteria.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria_split = pickle.load(open('data/trial_criteria_split.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Initial Concept Term Lists"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoker_list = ['Non-smoker', 'smoker', 'Current smoker', 'smoking', 'tobacco', 'nicotine',\n",
      "               'cigarettes']\n",
      "pregnancy_list = ['Pregnancy']\n",
      "birth_control_list = ['Birth control', 'contraception']\n",
      "drug_list = ['Illicit drugs', 'Alcohol abuse', 'illegal', 'illicit', 'drug abuse']\n",
      "heart_failure_list = ['Congestive Heart Failure', 'heart failure']\n",
      "hiv_list = ['HIV', 'aids', 'human immunodeficiency virus']\n",
      "allergy_list = ['Allergies', 'allergy', 'hypersensitivity']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inital Predictive Terms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoker_pred_list = ['current']\n",
      "pregnancy_pred_list = ['potential', 'negative']\n",
      "birth_control_pred_list = ['effective', 'Fertile patients', 'must use effective',\n",
      "                           'must use', 'use effective', 'Fertile patients must use',\n",
      "                           'fertile']\n",
      "drug_pred_list = ['use', 'abuse']\n",
      "heart_failure_pred_list = []\n",
      "hiv_pred_list = []\n",
      "allergy_pred_list = ['known', 'history', 'suspected', 'known suspected',\n",
      "                     'clinically significant']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria_split_test = deepcopy(dict(trial_criteria_split.items()[:3]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Process Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_text(text_dict):\n",
      "    #break sentences on '-' for each subdoc of each document\n",
      "    for key, doc in text_dict.items():\n",
      "        for n in xrange(len(doc)):\n",
      "            text_dict[key][n] = re.split(' - ', text_dict[key][n])\n",
      "\n",
      "\n",
      "    #get sentence tokenizer\n",
      "    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "    #run the sentence tokenizer over all the documents\n",
      "    def sent_token(text):\n",
      "        sentence_groups = []\n",
      "        for sent_group in text:\n",
      "            group_holder = []\n",
      "            for sent in sent_group:\n",
      "                group_holder += (sent_tokenizer.tokenize(sent))\n",
      "            sentence_groups.append(group_holder)\n",
      "            del group_holder\n",
      "        return sentence_groups\n",
      "\n",
      "    #run sentence tokenizer over each doc in the dict\n",
      "    for key, doc in text_dict.items():\n",
      "        text_dict[key] = sent_token(doc)\n",
      "\n",
      "\n",
      "\n",
      "    #CREATING TOKENS\n",
      "\n",
      "    #patter for tokenizing\n",
      "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
      "            ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\n",
      "            | \\w+([-\u2018]\\w+)*        # words with optional internal hyphens\n",
      "            | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
      "            | \\.\\.\\.            # ellipsis...   \n",
      "            | [][.,;\"'?():\\-_`]+  # these are separate tokens\n",
      "            '''\n",
      "\n",
      "\n",
      "\n",
      "    for key, doc in text_dict.items():\n",
      "        for n in xrange(len(doc)):\n",
      "            text_dict[key][n] = [nltk.regexp_tokenize(sent, pattern) for sent\n",
      "                                             in doc[n]]\n",
      "    return text_dict\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = process_text(trial_criteria_split_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save Tokenized Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(result, open('data/trial_criteria_split_token.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Tokenized Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria_split_token = pickle.load(open('data/trial_criteria_split_token.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "POS Tag Tokens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_tag(text_dict):\n",
      "    #tag document structured criteria text\n",
      "    def doc_tagger_pos(text):\n",
      "        result = []\n",
      "        for doc in text:\n",
      "            doc_text = []\n",
      "            for sent in doc:\n",
      "                doc_text.append(nltk.pos_tag(sent))\n",
      "            result.append(doc_text)\n",
      "        return result\n",
      "    \n",
      "    for key, doc in text_dict.items():\n",
      "        text_dict[key] = doc_tagger_pos(doc)\n",
      "        \n",
      "    return text_dict\n",
      "    \n",
      "result_pos = pos_tag(result)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save test corpus\n",
      "pickle.dump(result,open('data/test_tagged_data.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save tagged text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save tagged corpus\n",
      "pickle.save_object(criteria_text_sent_tag,\n",
      "                   'data/criteria_corpus_pos_tagged.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load tagged corpus\n",
      "criteria_text_sent_tag = pickle.open_object('data/criteria_corpus_pos_tagged.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}