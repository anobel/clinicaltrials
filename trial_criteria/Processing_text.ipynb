{
 "metadata": {
  "name": "",
  "signature": "sha256:e94f41c054b9a630b891e8411920b332f5a1e60c629e78f28d51faac30de2410"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import codecs\n",
      "import string\n",
      "import random\n",
      "from collections import Counter\n",
      "import cPickle as pickle\n",
      "import re\n",
      "from copy import deepcopy\n",
      "\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook was for developing the programs that had to be run on harbinger to process the full corpus of text. Because of the size it was too computationally intensive to run on my computer (the pos tagging took around 11 hours on harbinger to complete)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria = {}\n",
      "predefined_criteria = {}\n",
      "for row in codecs.open('data/clinical_study.txt','r','utf-8').readlines():\n",
      "    data = row.split('|')\n",
      "    trial_criteria[data[0]] = data[26]\n",
      "    # 27 = gender, 28 = min age, 29 = max age, 30 = Health volunteers\n",
      "    predefined_criteria[data[0]] = (data[27], data[28], data[29], data[30])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Split Inclusive and Exclusive"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_doc_on_exclusion(doc):\n",
      "    '''This function splits the criteria documents into the Inclusion \n",
      "    and Exclusion sections'''\n",
      "    split_doc = doc.split()\n",
      "    for idx, word in enumerate(split_doc):\n",
      "        if word.lower() == u'exclusion' and u'criteria' in split_doc[idx + 1].lower():\n",
      "            inclusive = ' '.join(split_doc[:idx])\n",
      "            exclusive =  ' '.join(split_doc[idx:])\n",
      "            return inclusive, exclusive\n",
      "    if split_doc[0].lower() == u'inclusion' and u'criteria' in split_doc[1].lower():\n",
      "        return doc, None\n",
      "\n",
      "no_inc_exc_dict = {}\n",
      "trial_criteria_split = {}\n",
      "\n",
      "for key, doc in trial_criteria.items():\n",
      "    try:\n",
      "        inclusive, exclusive = split_doc_on_exclusion(doc)\n",
      "        if exclusive is None:\n",
      "            trial_criteria_split[key] = [inclusive]\n",
      "        else:\n",
      "            trial_criteria_split[key] = [inclusive, exclusive]\n",
      "            \n",
      "    #if there are not inclusion/exclusion\n",
      "    except:\n",
      "         no_inc_exc_dict[key] = doc\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(trial_criteria_split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "152877"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save ram\n",
      "del trial_criteria"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "~10,000 trials did not have an inclusive exclusive split"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(no_inc_exc_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "10887"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(trial_criteria)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "163764"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save to pickle\n",
      "pickle.dump(predefined_criteria, open('data/predefined_criteria.pkl', 'wb'))\n",
      "pickle.dump(trial_criteria, open('data/trial_criteria.pkl', 'wb'))\n",
      "pickle.dump(trial_criteria_split, open('data/trial_criteria_split.pkl', 'wb'))\n",
      "pickle.dump(no_inc_exc_dict, open('data/no_inc_exc_dict.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data From Pickle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria = pickle.load(open('data/trial_criteria.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria_split = pickle.load(open('data/trial_criteria_split.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Initial Concept Term Lists"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoker_list = ['Non-smoker', 'smoker', 'Current smoker', 'smoking', 'tobacco', 'nicotine',\n",
      "               'cigarettes']\n",
      "pregnancy_list = ['Pregnancy']\n",
      "birth_control_list = ['Birth control', 'contraception']\n",
      "drug_list = ['Illicit drugs', 'Alcohol abuse', 'illegal', 'illicit', 'drug abuse']\n",
      "heart_failure_list = ['Congestive Heart Failure', 'heart failure']\n",
      "hiv_list = ['HIV', 'aids', 'human immunodeficiency virus']\n",
      "allergy_list = ['Allergies', 'allergy', 'hypersensitivity']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inital Predictive Terms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoker_pred_list = ['current']\n",
      "pregnancy_pred_list = ['potential', 'negative']\n",
      "birth_control_pred_list = ['effective', 'Fertile patients', 'must use effective',\n",
      "                           'must use', 'use effective', 'Fertile patients must use',\n",
      "                           'fertile']\n",
      "drug_pred_list = ['use', 'abuse']\n",
      "heart_failure_pred_list = []\n",
      "hiv_pred_list = []\n",
      "allergy_pred_list = ['known', 'history', 'suspected', 'known suspected',\n",
      "                     'clinically significant']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria_split_test = deepcopy(dict(trial_criteria_split.items()[:3]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Process Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_text(text_dict):\n",
      "    #break sentences on '-' for each subdoc of each document\n",
      "    for key, doc in text_dict.items():\n",
      "        for n in xrange(len(doc)):\n",
      "            text_dict[key][n] = re.split(' - ', text_dict[key][n])\n",
      "\n",
      "\n",
      "    #get sentence tokenizer\n",
      "    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "    #run the sentence tokenizer over all the documents\n",
      "    def sent_token(text):\n",
      "        sentence_groups = []\n",
      "        for sent_group in text:\n",
      "            group_holder = []\n",
      "            for sent in sent_group:\n",
      "                group_holder += (sent_tokenizer.tokenize(sent))\n",
      "            sentence_groups.append(group_holder)\n",
      "            del group_holder\n",
      "        return sentence_groups\n",
      "\n",
      "    #run sentence tokenizer over each doc in the dict\n",
      "    for key, doc in text_dict.items():\n",
      "        text_dict[key] = sent_token(doc)\n",
      "\n",
      "\n",
      "\n",
      "    #CREATING TOKENS\n",
      "\n",
      "    #patter for tokenizing\n",
      "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
      "            ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\n",
      "            | \\w+([-\u2018]\\w+)*        # words with optional internal hyphens\n",
      "            | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
      "            | \\.\\.\\.            # ellipsis...   \n",
      "            | [][.,;\"'?():\\-_`]+  # these are separate tokens\n",
      "            '''\n",
      "\n",
      "\n",
      "\n",
      "    for key, doc in text_dict.items():\n",
      "        for n in xrange(len(doc)):\n",
      "            text_dict[key][n] = [nltk.regexp_tokenize(sent, pattern) for sent\n",
      "                                             in doc[n]]\n",
      "    return text_dict\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = process_text(trial_criteria_split_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save Tokenized Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(result, open('data/trial_criteria_split_token.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Tokenized Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial_criteria_split_token = pickle.load(open('data/trial_criteria_split_token.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "POS Tag Tokens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_tag(text_dict):\n",
      "    #tag document structured criteria text\n",
      "    def doc_tagger_pos(text):\n",
      "        result = []\n",
      "        for doc in text:\n",
      "            doc_text = []\n",
      "            for sent in doc:\n",
      "                doc_text.append(nltk.pos_tag(sent))\n",
      "            result.append(doc_text)\n",
      "        return result\n",
      "    \n",
      "    for key, doc in text_dict.items():\n",
      "        text_dict[key] = doc_tagger_pos(doc)\n",
      "        \n",
      "    return text_dict\n",
      "    \n",
      "result_pos = pos_tag(result)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save test corpus\n",
      "pickle.dump(result,open('data/test_tagged_data.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save tagged text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save tagged corpus\n",
      "pickle.save_object(criteria_text_sent_tag,\n",
      "                   'data/criteria_corpus_pos_tagged.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load tagged corpus\n",
      "criteria_text_sent_tag = pickle.open_object('data/criteria_corpus_pos_tagged.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}